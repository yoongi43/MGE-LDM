
accelerator: gpu
devices: -1
precision: "16-mixed"

learning_rate: 1.5e-4
warmup_steps: 0
use_ema: true


strategy: ""
num_nodes: 1
num_gpus: 1
accumulate_grad_batches: 1
max_epochs: 100000
# gradient_clip_val: 5.0 ## Clipped at the pl pipeline


optimizer_configs:
  autoencoder:
    optimizer:
      type: "AdamW"
      config:
        betas: [0.8, 0.99]
        lr: 1.5e-4
        weight_decay: 0.001
    scheduler:
      type: "InverseLR"
      config:
        inv_gamma: 200000
        power: 0.5
        warmup: 0.999
  discriminator:
    optimizer:
      type: "AdamW"
      config:
        betas: [0.8, 0.99]
        lr: 3e-4
        weight_decay: 0.001
    scheduler:
      type: "InverseLR"
      config:
        inv_gamma: 200000
        power: 0.5
        warmup: 0.999

loss_configs:
  discriminator:
    type: "encodec"
    config:
      filters: 64
      n_ffts: [2048, 1024, 512, 256, 128]
      hop_lengths: [512, 256, 128, 64, 32]
      win_lengths: [2048, 1024, 512, 256, 128]
    weights:
      adversarial: 0.12
      feature_matching: 5.0
  spectral:
    type: "mrstft"
    config:
      fft_sizes: [2048, 1024, 512, 256, 128, 64, 32]
      hop_sizes: [512, 256, 128, 64, 32, 16, 8]
      win_lengths: [2048, 1024, 512, 256, 128, 64, 32]
      perceptual_weighting: true
    weights:
      mrstft: 1.0
  time:
    type: "l1"
    weights:
      l1: 0.0
  bottleneck:
    type: "kl"
    weights:
      kl: 1e-4

logging:
  log_every: 20

checkpoint:
  every_n_train_steps: 10000
  save_top_k: 1
  monitor: train/mrstft_loss
  save_last: true

demo:
  demo_every: 2000
  max_num_samples: 4

tqdm:
  refresh_rate: 10
      
